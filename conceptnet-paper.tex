\def\year{2017}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

% Packages added by Rob
% Also: grffile, latexsym, textcomp, longtable, booktabs, amsmath, amsfonts, amssymb?
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}%

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}
\begin{document}

\title{ConceptNet 5.5: An Open Multilingual Graph of General Knowledge}
\author{
    Robert Speer \\ Luminoso Technologies, Inc. \\ 675 Massachusetts Ave. \\ Cambridge, MA 02139
    \And
    Joshua Chin \\ Union College \\ 807 Union St. \\ Schenectady, NY 12308
}

\maketitle


\section{Abstract}\label{abstract}

Machine learning about language can be improved by supplying it with specific
knowledge and sources of external information. We present here a new version
of the linked open data resource ConceptNet that is particularly well suited
to be used with modern NLP techniques such as word embeddings.

ConceptNet is a knowledge graph that connects words and phrases of natural
language with labeled edges. Its knowledge is collected from many sources that
include expert-created resources, crowd-sourcing, and games with a purpose. It
is designed to represent the general knowledge involved in understanding
language, improving natural language applications by allowing the application
to better understand the meanings behind the words people use.

When ConceptNet is combined with word embeddings acquired from distributional
semantics (such as word2vec), it provides applications with understanding that
they would not acquire from distributional semantics alone, nor from narrower
resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art
results on intrinsic evaluations of word relatedness that translate into
improvements on extrinsic evaluations, including SAT-style analogies and story
understanding.


\section{Introduction}\label{introduction}

ConceptNet is a knowledge graph that connects words and phrases of
natural language (\emph{terms}) with labeled, weighted edges
(\emph{assertions}). The original release of ConceptNet \cite{liu2004conceptnet}
was intended as a parsed representation of Open Mind Common Sense
\cite{singh2002omcs}, a crowd-sourced knowledge project. This paper
describes the release of ConceptNet 5.5, which has expanded to include
lexical and world knowledge from many different sources in many
languages.

ConceptNet represents relations between words such as:

\begin{itemize}
    \item \emph{Fire} is \emph{hot}.
    \item A \emph{dog} has a \emph{tail}.
    \item A \emph{net} is used for \emph{catching fish}.
    \item ``\emph{Leaves}'' is a form of the word ``\emph{leaf}''.
    \item The word \emph{cold} in English is \emph{studený} in Czech.
    \item Um \emph{alimento} é usado para \emph{comer} [Food is used for eating].
\end{itemize}

In this paper, we will concisely assertions such as the above as triples of
their start node, relation label, and end node: the assertion that ``a dog has
a tail'' can be represented as (\emph{dog}, \emph{HasA}, \emph{tail}).

It also represents links between knowledge resources. In addition to its own
knowledge about the English term \emph{astronomy}, for example, ConceptNet
contains links to URLs that define \emph{astronomy} in WordNet, Wiktionary,
UMBEL, and DBPedia.

The graph-structured knowledge in ConceptNet can be particularly useful to NLP
learning algorithms, particularly those based on word embeddings, such as
\cite{mikolov2013word2vec}. We can use ConceptNet to build semantic spaces that
are more effective than distributional semantics alone.

The most effective semantic space is one that learns from both distributional
semantics and ConceptNet, using a generalization of the ``retrofitting'' method
\cite{faruqui2015retrofitting}. We call this hybrid semantic space ``ConceptNet
Numberbatch'', to clarify that it is a separate artifact from ConceptNet
itself.

ConceptNet Numberbatch performs significantly better than other systems across
many evaluations of word relatedness, and this increase in performance
translates to improvements on downstream tasks. We will show this by evaluating
it on the Story Cloze Test \cite{mostafazadeh2016cloze}, where a
straightforward ``bag of vectors'' from Numberbatch chooses the correct ending
to a simple story with 60\% accuracy, which is on par with the best known
system, a neural net specifically trained on the task. We also evaluate it on
SAT-style analogy questions, where its accuracy of 56.4\% is slightly higher
than the previous best system, LRA \cite{turney2005lra}, and only slightly
lower than the performance of the average human test-taker.

Building word embeddings is not the only application of ConceptNet, but it is a
way to apply ConceptNet that achieves clear benefits and is compatible with
ongoing research in distributional semantics. This shows the continued relevance of
ConceptNet, and of explicitly collecting knowledge to improve the computational
understanding of semantics.

In this paper, we will begin by describing ConceptNet 5.5 and its features,
show how to use ConceptNet alone as a semantic space and a measure of word
relatedness, and then proceed to describe and evaluate the hybrid system
ConceptNet Numberbatch on these various semantic tasks.

\section{Structure of ConceptNet}\label{structure-of-conceptnet}


\subsection{Knowledge sources}\label{knowledge-sources}

ConceptNet 5.5 is built from the following sources:

\begin{itemize}
\item
  Facts acquired from Open Mind Common Sense (OMCS) \cite{singh2002omcs}
\item
  Information extracted from parsing Wiktionary, in multiple languages,
  with a custom parser (``Wikiparsec'')
\item
  ``Games with a purpose'' designed to collect common knowledge,
  including Verbosity \cite{vonahn2006verbosity} in English, nadya.jp
  \cite{nakahara2011nadya} in Japanese, and the PTT Pet Game
  \cite{kuo2009petgame} in Chinese
\item
  Open Multilingual WordNet \cite{bond2013linking}, a linked-data
  representation of WordNet \cite{miller1998wordnet} and its parallel
  projects in multiple languages, representing word meanings as
  ``synsets'' of synonymous word senses
\item
  JMDict \cite{breen2004jmdict}, a Japanese-multilingual dictionary
\item
  OpenCyc \cite{matuszek2006cyc}, a hierarchy of hypernyms provided by
  Cyc, a system that represents common sense knowledge in predicate
  logic, and includes connections to the UMBEL ontology
  \cite{bergman2008umbel}
\item
  A subset of DBPedia \cite{auer2007dbpedia}, a network of facts
  extracted from Wikipedia infoboxes
\end{itemize}

With the combination of these sources, ConceptNet contains over 21
million edges and over 8 million nodes. Its English vocabulary contains
approximately 1,500,000 nodes, and there are 83 languages in which it
contains at least 10,000 nodes.

The largest source of input for ConceptNet is Wiktionary, which provides
18.1 million edges and is mostly responsible for its large multilingual
vocabulary. However, much of the character of ConceptNet comes from OMCS
and the various games with a purpose, which express many different kinds
of relations between terms, such as \emph{PartOf} (``a wheel is part of
a car'') and \emph{UsedFor} (``a car is used for driving'').


\subsection{Relations}\label{relations}

ConceptNet uses a closed class of selected relations such as \emph{IsA},
\emph{UsedFor}, and \emph{CapableOf}, intended to
represent a relationship independently of the language or the source of
the terms it connects.

ConceptNet 5.5 aims to align its knowledge resources on its core set of 36
relations. These generalized relations are similar in purpose to WordNet's
relations such as \emph{hyponym} and \emph{meronym}, as well as to the qualia
of the Generative Lexicon theory \cite{pustejovsky1991generative}.

To allow for expansion to new kinds of knowledge, a data
source can provide new relations that are considered provisional and
appear in a separate namespace. ConceptNet 5.5 includes 10 provisional
relations that come from DBPedia's much larger class of relations,
including \emph{dbpedia/knownFor} and \emph{dbpedia/genre}.

Relations with specific semantics, such as \emph{UsedFor} and
\emph{HasPrerequisite}, tend to connect common words and phrases, while
rarer words are connected by more general relations such as
\emph{Synonym} and \emph{RelatedTo}.

ConceptNet's edges are directed, but as a new feature in ConceptNet 5.5,
some relations are designated as being symmetric: \emph{SimilarTo},
\emph{RelatedTo}, \emph{EtymologicallyRelatedTo}, \emph{Synonym},
\emph{Antonym}, and \emph{DistinctFrom}. The directionality of these
edges is unimportant. The assertion (\emph{A}, \emph{SimilarTo},
\emph{B}) is considered equivalent to (\emph{B}, \emph{SimilarTo},
\emph{A}).


\subsection{Term representation}\label{term-representation}

ConceptNet represents terms in a standardized form. The text is
Unicode-normalized (as NFKC) and lowercased, and split into
non-punctuation tokens using a generalization of the Unicode word
segmentation algorithm. The tokens are joined with underscores, and this
text is prepended with the URI \texttt{/c/lang}, where \emph{lang} is
the BCP 47 language code for the language the term is in. As an example,
the English term ``United States'' becomes
\texttt{/c/en/united\_states}. Relations have a separate namespace of
URIs prefixed with \texttt{/r}, such as \texttt{/r/PartOf}.

The particular tokenizer used is the default tokenizer in the Python
package \texttt{wordfreq}, version 1.5. Its most notable difference from
Unicode tokenization is that it does not insert word breaks between all
Han or Thai characters, whereas the Unicode algorithm would.

% Often, in multilingual code, one must make a choice about how coarsely
% to represent languages, because languages and dialects can have varying
% degrees of mutual intelligibility and partially but not entirely
% overlapping vocabularies. Examples include whether to distinguish
% Simplified from Traditional Chinese, Norwegian Bokmål from Nynorsk,
% Bosnian from Croatian from Serbian, and regional variants of Arabic such
% as Egyptian Arabic from Modern Standard Arabic. ConceptNet makes the
% decision to use the coarsest language codes in all of these cases:
% \texttt{zh} for all variants of Chinese, \texttt{no} for Norwegian,
% \texttt{sh} for Serbo-Croatian, \texttt{ar} for Arabic, and so on. The
% motivation is that it's better to include additional terms from a
% related language than to leave gaps in a language's vocabulary. This
% additionally avoids the need to disambiguate data sources that also use
% coarse language codes, such as Serbo-Croatian entries in Wiktionary.

Previous versions of ConceptNet required terms in English to be in lemmatized
form, so that, for example, ``drive'' and ``driving'' would be represented as
the same term, allowing the assertions (\emph{car}, \emph{UsedFor},
\emph{driving}) and (\emph{drive}, \emph{HasPrerequisite}, \emph{have license})
to be connected. With lemmatization, the term ``United States'' was represented
as the confusing-looking \texttt{/c/en/unite\_state}, looking up terms required
ConceptNet-specific preprocessing, and some terms with ambiguous lemmatization
were difficult to look up. ConceptNet 5.5 removes the lemmatizer, and instead
relates inflections of words using the \emph{FormOf} relation. The two
assertions above are now linked by the third assertion (\emph{driving},
\emph{FormOf}, \emph{drive}), and both ``driving'' and ``drive'' can be looked
up in ConceptNet.

One English-specific normalization remains, because English terms are more
likely to come from free text than other languages. A small set of stopwords is
removed from all English terms: ``a'', ``an'', ``the'', and as the first word
of a term, ``to''. Removing other stopwords is an option left up to the
importer for a particular data source.


\subsection{Vocabulary}\label{vocabulary}

When building a knowledge graph, the decision of what a node should
represent has significant effects on how the graph is used. It also has
implications that can make linking and importing other resources
non-trivial, because different resources make different decisions about
their representation.

In ConceptNet, a node is a word or phrase of a natural language, often a
common word in its undisambiguated form. The word ``lead'' in English is
a term in ConceptNet, represented by the URI \texttt{/c/en/lead}, even
though it has multiple meanings. The advantage of ambiguous terms is
that they can be extracted easily from natural language, which is also
ambiguous. This representation is equivalent to that used by systems
that learn distributional semantics from text, such as word2vec
\cite{mikolov2013word2vec}.

ConceptNet imports knowledge from some other systems, but this usually
is not as straightforward as, for example, simply including WordNet as a
subgraph of ConceptNet. These other systems have their own target
vocabularies that need to be aligned with ConceptNet, which is a
somewhat lossy operation. WordNet's nodes are synsets -- sets of senses
of words that are assumed to be interchangeable, even though they may
have different connotations. DBPedia's nodes are titles of encyclopedia
articles, which are generally either specific named entities or topic
areas such as ``Astronomy''. OpenCyc's nodes are a set of logical
predicates, with heuristically-generated mappings to natural language
phrases. All of these have an underspecified many-to-many mapping to
ConceptNet terms, and importing them thus requires heuristics to
transform them into ConceptNet's vocabulary.

A term that is imported from another knowledge graph will be connected
to ConceptNet nodes via the \texttt{ExternalURL} ConceptNet relation.
(This is similar in spirit to the OWL \texttt{sameAs} relation, but
\texttt{sameAs} is not suited for a many-to-many mapping because it is
supposed to be transitive.) This preserves the provenance of the data
and enables looking up what the untransformed data was, and also allows
ConceptNet to participate in the broader ecosystem of Linked Open Data.

\subsection{Word senses}\label{word-senses}

ConceptNet's representation allows for more specific, disambiguated
versions of a term. The URI \texttt{/c/en/lead/n} refers to noun senses
of the word ``lead'', and is effectively included within
\texttt{/c/en/lead} when searching or traversing ConceptNet, and
linked to it with the implicit relation \emph{SenseOf}. Many data
sources provide information about parts of speech, allowing us to use
this as a common representation that provides a small amount of
disambiguation.

We have experimented with further disambiguating individual word senses, as in
\texttt{/c/en/lead/n/metal}, following up on work that showed that word senses
can be disambiguated by finding the best match to nearby words on features
including ConceptNet \cite{havasi2010coarse}. However, we have not found a
satisfying vocabulary of distinct word senses to use for this purpose. So far,
we have settled for disambiguating only the part of speech, which is also the
representation of word senses targeted by the \emph{sense2vec} system
\cite{trask2015sense2vec}.


\subsection{Tracking the provenance of knowledge}\label{provenance}

Each assertion contains extra data for tracking the provenance of the
knowledge it contains, which is used to find systematic errors and bad
contributions. Sources are represented by URIs and divided into multiple
types. The prefix \texttt{/s/contributor} represents individual people
who contributed knowledge to OMCS or one of its sister projects.
\texttt{/s/activity} represents what a contributor was doing when they
contributed the knowledge, such as responding to a prompt or playing a
game with a purpose. \texttt{/s/resource} represents an existing
knowledge resource, such as WordNet or Wiktionary. \texttt{/s/process}
represents a process for transforming knowledge, such as the Wiktionary
parser or a rule for fixing the parsing of prepositions in OMCS.

The provenance of an assertion is a set of sets of sources. The inner
sets indicate multiple sources that were used together to create the
assertion: for example, a contributor, an activity, and a process may
all be involved in collecting one assertion. Each inner set of sources
contributes some amount of weight to the assertion. This number is
provided by the import scripts for each knowledge source, which are
asked to estimate the reliability of each assertion they produce.

The outer set represents sources that separately provided the same
assertion. When there are multiple separate sources, they add their
weights together, indicating that the assertion is more reliable because
we know it for multiple reasons.


% \subsection{The use-mention distinction}

% ConceptNet does not strictly enforce the use-mention distinction. Some
% of its assertions, such as (\emph{cat}, \emph{IsA}, \emph{animal}),
% relate the meanings of the words as they are used, while others, such as
% (\emph{cat}, \emph{DerivedFrom}, \emph{cattus}), mention the words as
% objects in themselves. However, it is intended that all relations,
% including these ``mention'' relations, informatively represent something
% about the meanings of the words. The etymology of a word, as indicated
% by the \emph{DerivedFrom} or \emph{EtymologicallyRelatedTo} relations,
% is often a clue to its meaning.

% Unacceptable assertions would include (\emph{cat}, \emph{HasA},
% \emph{three letters}) or (\emph{noon}, \emph{IsA}, \emph{palindrome}).
% Sometimes these appear in data sources, particularly games with a
% purpose, whose players are tempted to use wordplay to achieve their
% goals. The code that imports data from the word game Verbosity
% heuristically detects wordplay, and mentions of words as words, so that
% it can filter them out.

\section{Related work}

% WordNet
% Freebase / Google Knowledge Graph
% DBPedia
% Retrofitting
% word2vec
% GloVe
% Levy, Hyperwords


\section{Finding related words using ConceptNet}
\label{finding-related-words-using-conceptnet}

It is important to note that ConceptNet is not just a package of word
embeddings; it consists of structured knowledge that can be used in many
representations.  Word embeddings are, however, an easily applicable
representation of semantics that can benefit noticeably from the knowledge
contained in ConceptNet. We will show, via several standard evaluations, that
word embeddings that are informed by ConceptNet are more effective than those
inferred only from the distributional semantics of a corpus of text.

\subsection{Building word embeddings from ConceptNet}
\label{pruning}
[...] % Describe ConceptNet PPMI here

To save space and time when computing, we prune the poorly-connected fringe of
the ConceptNet graph: terms that are connected to fewer than 3 edges. These
terms would have only a weak effect on the semantic spaces we compute from
ConceptNet. We still include them in the vocabulary by assigning them vectors
after the fact, as the average of the vectors of the terms they are connected
to.

\subsection{Combining ConceptNet with distributional word embeddings}
% Expanded retrofitting and redecomposition

\section{Evaluation methods}

[...]

To compare the word-embedding ensemble presented in this paper (ConceptNet
Numberbatch) to other systems, we will first compare results on intrinsic
evaluations of the word embeddings themselves, involving word relatedness and
proportional analogies. We will also use an extrinsic evaluation of story
understanding, showing that better embeddings lead to better performance on a
linguistic task.

\subsection{Intrinsic evaluations of word relatedness}
\label{intrinsic-evaluations}
% Word relatedness and analogy evaluations

One way to evaluate the intrinsic performance of a semantic space is to ask it
to rank the relatedness of pairs of words, and compare its judgments to human
judgments.\footnote{It is sometimes important to distinguish \emph{similarity}
from \emph{relatedness}. For example, the term ``coffee'' is related to
``mug'', but coffee is not \emph{similar} to a mug. What a machine can learn
from the connectivity of ConceptNet is focused on relatedness.} If one word in
a pair is out-of-vocabulary, the pair is assumed to have a relatedness of 0. A
good semantic space will provide a ranking of relatedness that is highly
correlated with the human gold-standard ranking, as measured by its Spearman
correlation ($\rho$).

Many gold standards of word relatedness are in common use. Here, we focus on
MEN-3000 \cite{bruni2014men}, a large crowd-sourced ranking of common words; RW
\cite{luong2013rw}, a ranking of rare words; and MTurk-771
\cite{halawi2012mturk}, another crowd-sourced evaluation of a variety of words.
To avoid manually overfitting by designing our semantic space around a
particular evaluation, we experimented using smaller development sets, holding
out some test data until it was time to include results in this paper:

\begin{itemize}
\item
    MEN-3000 is already divided into a 2000-item development set and a
    1000-item test set. We use the results from the test set as the final results.
\item
    RW has no standard dev/test breakdown. We sampled 2/3 of its items as
    a development set and held out the other 1/3 (every third line of the file,
    starting with the third). However, we report results on the complete set,
    for consistency with other publications.
\item
    We did not use MTurk-771 in development, holding out the entire set
    as a final test, showing that ConceptNet Numberbatch performs well on a
    previously-unseen vocabulary.
\end{itemize}


[...]

Previous work that combines ConceptNet in an ensemble with other word
embeddings \cite{speer2016ensemble} has included a time-consuming step of
unifying the vocabularies of multiple resources and interpolating what the
missing word embeddings should be. With ConceptNet 5.5, we found this
unnecessary.  The vocabulary of ConceptNet 5.5 is sufficient to cover all but
four words that appear anywhere in the English test data.\footnote{We are
content to return zero-vectors for the four out-of-vocabulary words, which are
``amazings'', ``internationaler'', ``liverpools'', and ``discoverys''. All of
these are from the Rare Words evaluation, and are probably the result of
imperfect machine-generation of examples.}

\subsection{Intrinsic evaluations of analogies}

Proportional analogies are statements of the form ``$a_1$ is to $b_1$ as $a_2$
is to $b_2$''. The task of filling in missing values of a proportional analogy
was common until recently on standardized tests such as the SAT. Now, it is
popular as a way to show that a semantic space can approximate relationships
between words, even without taking explicit relationships into account.

Much of the groundwork for evaluating systems' ability to solve proportional
analogies was laid by Peter Turney, including his method of Latent Relational
Analysis \cite{turney2005lra}, which was quite effective at solving
proportional analogies by repeatedly searching the Web for the words involved
in them.  A newer method called SuperSim \cite{turney2013distributional} does
not require Web searching. These methods are evaluated on a dataset of 374 SAT
questions that Turney and his collaborators have collected, which they provide
on request.  Unfortunately, the questions are not licensed for free
distribution.

[... describe our pairwise analogy heuristic ...]

Many systems now are instead evaluated on a synthetic dataset that was packaged
with Google's word2vec, which tests a much smaller variety of analogies focused
on syntactic changes to words, gendered words, and geographic facts. The
appropriateness of this dataset for measuring vector spaces has been questioned
\cite{linzen2016issues}, based on the variability of the results and the fact
that many of the analogies can be solved while ignoring one or more terms in
the query.

We will evaluate our system on the Google analogies simply to allow comparisons
to other systems that focus on this evaluation. Levy showed that comparing word
similarities with the 3CosMul heuristic \cite{levy2015embeddings} performs best
on this data, and made his results easily reproducible and comparable by
providing evaluation software called Hyperwords. We will use Hyperwords in this
paper to compare systems on the Google analogy task, as well as the purely
syntactic MSR task.

The Google and MSR evaluations test whether a system returns the single word
deemed to be the correct completion of an analogy, out of its entire
vocabulary. The results are thus sensitive to the size of the vocabulary, and
benefit from a limited vocabulary that includes enough of the prompt words.
When exporting ConceptNet Numberbatch to the Hyperwords format, we select the
200,000 most common English words, according to the \emph{wordfreq} data set
\cite{speer2016wordfreq}, that appear in the pruned vocabulary of ConceptNet
(see ``Building word embeddings from ConceptNet'').

\subsection{An extrinsic evaluation of story understanding}
\label{extrinsic-evaluation}
% Story Cloze Test


% Paragraphs that were cut, but we might include if there's space:
%
% WordNet has a fine-grained vocabulary of word senses that are
% distinguished by the synsets that they participate in. Each language of
% Wiktionary has its own vocabulary of word senses; the English Wiktionary
% actually has two different word sense vocabularies, one used for
% definitions and one used for translations. Wikipedia disambiguation
% pages are another vocabulary of word senses that can be applied to
% resources derived from it, such as DBPedia. Granularity can make
% vocabularies difficult to align: the English word ``run'' has 57 senses
% in WordNet, 17 in the French Wiktionary, and 4 in the German Wiktionary.
% The English Wiktionary has 45 senses of ``run'' that are used in
% translations, which map in an underspecified way to at least 96
% definitions.

% Sometimes this alignment involves using extra information that
% incidentally comes with the data. To include WordNet, we need to assign
% each synset a primary label that will be used in its semantic relations,
% and then link other synonyms in the set to that label using the
% \emph{Synonym} relation. In WordNet RDF 3.1, the labels are unordered,
% although some terms are more appropriate as primary labels than others.
% Fortunately, WordNet RDF 2.0, which is linked to the current WordNet RDF
% 3.1 with \texttt{owl\#SameAs} links, also assigned primary labels to
% synsets by putting these labels in their URIs. So when the synset at
% http://wordnet-rdf.princeton.edu/wn31/100028005-n is linked to
% http://www.w3.org/2006/03/wn/wn20/instances/synset-shape-noun-2, we
% conclude that its primary label is ``shape''. This is one of many
% heuristics that are tried when importing WordNet synsets into
% ConceptNet.

\bibliographystyle{aaai}
\bibliography{conceptnet-paper}

\end{document}

